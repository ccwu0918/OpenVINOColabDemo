{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccwu0918/OpenVINOColabDemo/blob/main/102_pytorch_onnx_to_openvino.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ccwu0918/OpenVINOColabDemo/blob/main/102_pytorch_onnx_to_openvino.ipynb)"
      ],
      "metadata": {
        "id": "LY--S5QpuyFt"
      },
      "id": "LY--S5QpuyFt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Openvino Environment Test "
      ],
      "metadata": {
        "id": "Oc3NrvZFZO4E"
      },
      "id": "Oc3NrvZFZO4E"
    },
    {
      "cell_type": "code",
      "source": [
        "!cpuinfo"
      ],
      "metadata": {
        "id": "lkwmRJEUZWRI"
      },
      "execution_count": null,
      "outputs": [],
      "id": "lkwmRJEUZWRI"
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "id": "ZfEc5c6ij3YJ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ZfEc5c6ij3YJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install OpenVINOâ„¢ Notebooks\n"
      ],
      "metadata": {
        "id": "aIHItyYmfDv3"
      },
      "id": "aIHItyYmfDv3"
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone --depth=1 https://github.com/openvinotoolkit/openvino_notebooks.git "
      ],
      "metadata": {
        "id": "5s2i4ZTDfDPr"
      },
      "id": "5s2i4ZTDfDPr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !sed -i 's/Pillow>=8.3.2/Pillow==9.0.0/g' ./openvino_notebooks/requirements.txt\n",
        "# !sed -i 's/matplotlib<3.4/\\matplotlib==3.1.3/g' ./openvino_notebooks/requirements.txt"
      ],
      "metadata": {
        "id": "fzhQz0XefKL_"
      },
      "id": "fzhQz0XefKL_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i '/Pillow>=8.3.2/d' ./openvino_notebooks/requirements.txt\n",
        "!sed -i '/matplotlib<3.4/d' ./openvino_notebooks/requirements.txt"
      ],
      "metadata": {
        "id": "iVmrglAPfKuq"
      },
      "id": "iVmrglAPfKuq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!pip install --upgrade -r ./openvino_notebooks/requirements.txt"
      ],
      "metadata": {
        "id": "9gQjKkxifOTc"
      },
      "id": "9gQjKkxifOTc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ” Restarting Kernel"
      ],
      "metadata": {
        "id": "PUiGAxexgQOV"
      },
      "id": "PUiGAxexgQOV"
    },
    {
      "cell_type": "code",
      "source": [
        "## !git clone https://github.com/facebookresearch/detectron2 detectron2_repo\n",
        "## !pip install -e detectron2_repo\n",
        "\n",
        "print(\"ðŸ” Restarting kernel...\")  \n",
        "print(\"Runtime is now restarting...\")\n",
        "print('Stopping RunTime! Please run again.')  \n",
        "# print(\"You can ignore the error message [Your session crashed for an unknown reason.]\")\n",
        "import os\n",
        "# os._exit(0)  # restart\n",
        "os.kill(os.getpid(), 9)\n",
        "get_ipython().kernel.do_shutdown(True)\n",
        "# exit()"
      ],
      "metadata": {
        "id": "Q2AWKG3ggStu"
      },
      "id": "Q2AWKG3ggStu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!source /opt/intel/openvino_2022/setupvars.sh"
      ],
      "metadata": {
        "id": "ym4AyNTDfQyl"
      },
      "id": "ym4AyNTDfQyl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Open Model Zoo"
      ],
      "metadata": {
        "id": "kdKpae0tfHAX"
      },
      "id": "kdKpae0tfHAX"
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone --depth=1 https://github.com/openvinotoolkit/open_model_zoo.git\n",
        "%cd ./open_model_zoo\n",
        "!git submodule update --init --recursive\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "8a8YQO-ofT8H"
      },
      "id": "8a8YQO-ofT8H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python -mpip install --user -r ./open_model_zoo/demos/requirements.txt"
      ],
      "metadata": {
        "id": "fN6CLfOcf1QB"
      },
      "id": "fN6CLfOcf1QB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !ls /root/.local/bin -al"
      ],
      "metadata": {
        "id": "2aTawMIxdHnz"
      },
      "execution_count": null,
      "outputs": [],
      "id": "2aTawMIxdHnz"
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp /root/.local/bin/*  /usr/local/bin/"
      ],
      "metadata": {
        "id": "T4pPf3V-_GMa"
      },
      "execution_count": null,
      "outputs": [],
      "id": "T4pPf3V-_GMa"
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install /content/open_model_zoo/demos/common/python"
      ],
      "metadata": {
        "id": "9nqvRI6Tf2ci"
      },
      "id": "9nqvRI6Tf2ci",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Install OpenCV"
      ],
      "metadata": {
        "id": "2cfb1z7Sf3wW"
      },
      "id": "2cfb1z7Sf3wW"
    },
    {
      "cell_type": "code",
      "source": [
        "# !sudo -E /opt/intel/openvino_2022/extras/scripts/download_opencv.sh"
      ],
      "metadata": {
        "id": "avW87FADf7e8"
      },
      "id": "avW87FADf7e8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenVINO Sample Images"
      ],
      "metadata": {
        "id": "2XF6dbAqgi95"
      },
      "id": "2XF6dbAqgi95"
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/sample-images\n",
        "%cd /content/sample-images\n",
        "!wget https://github.com/alihussainia/openvino-colab/raw/master/demo_files/images/blue-car.jpg\n",
        "!wget https://github.com/alihussainia/openvino-colab/raw/master/demo_files/images/sitting-on-car.jpg\n",
        "!wget https://github.com/alihussainia/openvino-colab/raw/master/demo_files/images/sign.jpg\n",
        "!wget https://user-images.githubusercontent.com/2350015/97497833-66185d80-196b-11eb-8c96-ce7ed676cbd6.jpg\n",
        "!mv 97497833-66185d80-196b-11eb-8c96-ce7ed676cbd6.jpg dog.jpg\n",
        "!wget https://obs.line-scdn.net/0hVK227DTKCWQPSx0dLTZ2MzcdBRU8LRNtLShDAH9NUARyZx46YC5aByseB0gqeEYyL3hCASxDVwMhfxlmYw/w1200 \n",
        "!mv w1200 people.jpg"
      ],
      "metadata": {
        "id": "PaSPBbXNgk_J"
      },
      "id": "PaSPBbXNgk_J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ä»¥OpenCVæª¢è¦–è¼¸å…¥å½±åƒ\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "img = cv2.imread('/content/sample-images/sitting-on-car.jpg') # è®€å…¥æŒ‡å®šå½©è‰²å½±åƒ\n",
        "cv2_imshow(img)  # é¡¯ç¤ºå½±åƒ"
      ],
      "metadata": {
        "id": "BlWitgr3gxJa"
      },
      "id": "BlWitgr3gxJa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/openvino_notebooks/notebooks/102-pytorch-onnx-to-openvino"
      ],
      "metadata": {
        "id": "ns-EeRPrm4Ph"
      },
      "id": "ns-EeRPrm4Ph",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "af86d01b",
      "metadata": {
        "id": "af86d01b"
      },
      "source": [
        "# Convert a PyTorch Model to ONNX and OpenVINO IR\n",
        "\n",
        "This tutorial demonstrates step-by-step instructions to perform inference on a PyTorch semantic segmentation model using OpenVINO's Inference Engine.\n",
        "\n",
        "First, the PyTorch model is converted to [ONNX](https://onnx.ai/) and OpenVINO Intermediate Representation (IR) formats. Then the ONNX and IR models are loaded in OpenVINO Inference Engine to show model predictions. The model is pre-trained on the [CityScapes](https://www.cityscapes-dataset.com) dataset. The source of the model is [FastSeg](https://github.com/ekzhang/fastseg)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cdf8008",
      "metadata": {
        "id": "4cdf8008"
      },
      "source": [
        "## Preparation\n",
        "\n",
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2573d828",
      "metadata": {
        "id": "2573d828"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from IPython.display import Markdown, display\n",
        "from fastseg import MobileV3Large\n",
        "from openvino.runtime import Core\n",
        "\n",
        "# sys.path.append(\"../utils\") -->\n",
        "sys.path.append(\"/content/openvino_notebooks/notebooks/utils\")\n",
        "from notebook_utils import CityScapesSegmentation, segmentation_map_to_image, viz_result_image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37b80fe4",
      "metadata": {
        "id": "37b80fe4"
      },
      "source": [
        "### Settings\n",
        "\n",
        "Set the name for the model, and the image width and height that will be used for the network. CityScapes is pretrained on images of 2048x1024. Using smaller dimensions will impact model accuracy, but will improve inference speed. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a9303a8",
      "metadata": {
        "id": "3a9303a8"
      },
      "outputs": [],
      "source": [
        "IMAGE_WIDTH = 1024  # Suggested values: 2048, 1024 or 512. The minimum width is 512.\n",
        "# Set IMAGE_HEIGHT manually for custom input sizes. Minimum height is 512\n",
        "IMAGE_HEIGHT = 1024 if IMAGE_WIDTH == 2048 else 512\n",
        "DIRECTORY_NAME = \"model\"\n",
        "BASE_MODEL_NAME = DIRECTORY_NAME + f\"/fastseg{IMAGE_WIDTH}\"\n",
        "\n",
        "# Paths where PyTorch, ONNX and OpenVINO IR models will be stored\n",
        "model_path = Path(BASE_MODEL_NAME).with_suffix(\".pth\")\n",
        "onnx_path = model_path.with_suffix(\".onnx\")\n",
        "ir_path = model_path.with_suffix(\".xml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b33033e",
      "metadata": {
        "id": "2b33033e"
      },
      "source": [
        "### Download the Fastseg Model\n",
        "\n",
        "Download, load and save the model with pretrained weights. This may take some time if you have not downloaded the model before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9600481",
      "metadata": {
        "id": "c9600481"
      },
      "outputs": [],
      "source": [
        "print(\"Downloading the Fastseg model (if it has not been downloaded before)....\")\n",
        "model = MobileV3Large.from_pretrained().cpu().eval()\n",
        "print(\"Loaded PyTorch Fastseg model\")\n",
        "\n",
        "# Save the model\n",
        "model_path.parent.mkdir(exist_ok=True)\n",
        "torch.save(model.state_dict(), str(model_path))\n",
        "print(f\"Model saved at {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bad92bb9",
      "metadata": {
        "id": "bad92bb9"
      },
      "source": [
        "## ONNX Model Conversion\n",
        "\n",
        "### Convert PyTorch model to ONNX\n",
        "\n",
        "The output for this cell will show some warnings. These are most likely harmless. Conversion succeeded if the last line of the output says `ONNX model exported to fastseg1024.onnx.` "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "659aeac7",
      "metadata": {
        "id": "659aeac7"
      },
      "outputs": [],
      "source": [
        "if not onnx_path.exists():\n",
        "    dummy_input = torch.randn(1, 3, IMAGE_HEIGHT, IMAGE_WIDTH)\n",
        "\n",
        "    # For the Fastseg model, setting do_constant_folding to False is required\n",
        "    # for PyTorch>1.5.1\n",
        "    torch.onnx.export(\n",
        "        model,\n",
        "        dummy_input,\n",
        "        onnx_path,\n",
        "        opset_version=11,\n",
        "        do_constant_folding=False,\n",
        "    )\n",
        "    print(f\"ONNX model exported to {onnx_path}.\")\n",
        "else:\n",
        "    print(f\"ONNX model {onnx_path} already exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b490241b",
      "metadata": {
        "id": "b490241b"
      },
      "source": [
        "### Convert ONNX Model to OpenVINO IR Format\n",
        "\n",
        "Call the OpenVINO Model Optimizer tool to convert the ONNX model to OpenVINO IR with FP16 precision. The models are saved to the current directory. We add the mean values to the model and scale the output with the standard deviation with `--scale_values`. With these options, it is not necessary to normalize input data before propagating it through the network.\n",
        "\n",
        "See the [Model Optimizer Developer Guide](https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) for more information about Model Optimizer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f99ea10",
      "metadata": {
        "id": "3f99ea10"
      },
      "source": [
        "Executing this command may take a while. There may be some errors or warnings in the output. Model Optimization was successful if the last lines of the output include `[ SUCCESS ] Generated IR version 11 model.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c1e8029",
      "metadata": {
        "tags": [],
        "id": "5c1e8029"
      },
      "outputs": [],
      "source": [
        "# Construct the command for Model Optimizer\n",
        "mo_command = f\"\"\"mo\n",
        "                 --input_model \"{onnx_path}\"\n",
        "                 --input_shape \"[1,3, {IMAGE_HEIGHT}, {IMAGE_WIDTH}]\"\n",
        "                 --mean_values=\"[123.675, 116.28 , 103.53]\"\n",
        "                 --scale_values=\"[58.395, 57.12 , 57.375]\"\n",
        "                 --data_type FP16\n",
        "                 --output_dir \"{model_path.parent}\"\n",
        "                 \"\"\"\n",
        "mo_command = \" \".join(mo_command.split())\n",
        "print(\"Model Optimizer command to convert the ONNX model to OpenVINO:\")\n",
        "display(Markdown(f\"`{mo_command}`\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e48d30a7",
      "metadata": {
        "id": "e48d30a7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "if not ir_path.exists():\n",
        "    print(\"Exporting ONNX model to IR... This may take a few minutes.\")\n",
        "    mo_result = %sx $mo_command\n",
        "    print(\"\\n\".join(mo_result))\n",
        "else:\n",
        "    print(f\"IR model {ir_path} already exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "608279bb",
      "metadata": {
        "id": "608279bb"
      },
      "source": [
        "## Show Results\n",
        "\n",
        "Confirm that the segmentation results look as expected, by comparing model predictions on the ONNX, IR and PyTorch model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08d2a742",
      "metadata": {
        "id": "08d2a742"
      },
      "source": [
        "### Load and Preprocess an Input Image\n",
        "\n",
        "For the OpenVINO model, normalization is moved to the model. For the ONNX and PyTorch models, images need to be normalized before propagating through the network. A sample image from the [Mapillary Vistas](https://www.mapillary.com/dataset/vistas) dataset is provided for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "738cb5e7",
      "metadata": {
        "id": "738cb5e7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def normalize(image: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Normalize the image to the given mean and standard deviation\n",
        "    for CityScapes models.\n",
        "    \"\"\"\n",
        "    image = image.astype(np.float32)\n",
        "    mean = (0.485, 0.456, 0.406)\n",
        "    std = (0.229, 0.224, 0.225)\n",
        "    image /= 255.0\n",
        "    image -= mean\n",
        "    image /= std\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b92da2c",
      "metadata": {
        "id": "8b92da2c",
        "tags": []
      },
      "outputs": [],
      "source": [
        "image_filename = \"data/street.jpg\"\n",
        "# image_filename = \"/content/sample-images/people.jpg\"\n",
        "# image_filename = \"/content/sample-images/sitting-on-car.jpg\"\n",
        "image = cv2.cvtColor(cv2.imread(image_filename), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "resized_image = cv2.resize(image, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
        "normalized_image = normalize(resized_image)\n",
        "\n",
        "# Convert the resized images to network input shape\n",
        "input_image = np.expand_dims(np.transpose(resized_image, (2, 0, 1)), 0)\n",
        "normalized_input_image = np.expand_dims(np.transpose(normalized_image, (2, 0, 1)), 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57bab671",
      "metadata": {
        "id": "57bab671"
      },
      "source": [
        "### Load the OpenVINO IR Network and Run Inference on the ONNX model\n",
        "\n",
        "Inference Engine can load ONNX models directly. We first load the ONNX model, do inference and show the results. After that we load the model that was converted to Intermediate Representation (IR) with Model Optimizer and do inference on that model and show the results on an image from [Mapillary Vistas](https://www.mapillary.com/dataset/vistas).\n",
        "\n",
        "#### 1. ONNX Model in Inference Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "194b8bc2",
      "metadata": {
        "id": "194b8bc2",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Load network to Inference Engine\n",
        "ie = Core()\n",
        "model_onnx = ie.read_model(model=onnx_path)\n",
        "compiled_model_onnx = ie.compile_model(model=model_onnx, device_name=\"CPU\")\n",
        "\n",
        "output_layer_onnx = compiled_model_onnx.output(0)\n",
        "\n",
        "# Run inference on the input image\n",
        "res_onnx = compiled_model_onnx([normalized_input_image])[output_layer_onnx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "064a14ba",
      "metadata": {
        "id": "064a14ba",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Convert network result to segmentation map and display the result\n",
        "result_mask_onnx = np.squeeze(np.argmax(res_onnx, axis=1)).astype(np.uint8)\n",
        "viz_result_image(\n",
        "    image,\n",
        "    segmentation_map_to_image(result_mask_onnx, CityScapesSegmentation.get_colormap()),\n",
        "    resize=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70c0e7d8",
      "metadata": {
        "id": "70c0e7d8"
      },
      "source": [
        "#### 2. IR Model in Inference Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b628c3a4",
      "metadata": {
        "id": "b628c3a4",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Load the network in Inference Engine\n",
        "ie = Core()\n",
        "model_ir = ie.read_model(model=ir_path)\n",
        "compiled_model_ir = ie.compile_model(model=model_ir, device_name=\"CPU\")\n",
        "\n",
        "# Get input and output layers\n",
        "output_layer_ir = compiled_model_ir.output(0)\n",
        "\n",
        "# Run inference on the input image\n",
        "res_ir = compiled_model_ir([input_image])[output_layer_ir]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e625d64a",
      "metadata": {
        "id": "e625d64a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "result_mask_ir = np.squeeze(np.argmax(res_ir, axis=1)).astype(np.uint8)\n",
        "viz_result_image(\n",
        "    image,\n",
        "    segmentation_map_to_image(result=result_mask_ir, colormap=CityScapesSegmentation.get_colormap()),\n",
        "    resize=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b8de873",
      "metadata": {
        "id": "2b8de873"
      },
      "source": [
        "## PyTorch Comparison\n",
        "\n",
        "Do inference on the PyTorch model to verify that the output visually looks the same as the output on the ONNX/IR models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44b4f32b",
      "metadata": {
        "id": "44b4f32b",
        "tags": []
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    result_torch = model(torch.as_tensor(normalized_input_image).float())\n",
        "\n",
        "result_mask_torch = torch.argmax(result_torch, dim=1).squeeze(0).numpy().astype(np.uint8)\n",
        "viz_result_image(\n",
        "    image,\n",
        "    segmentation_map_to_image(result=result_mask_torch, colormap=CityScapesSegmentation.get_colormap()),\n",
        "    resize=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9feb7b4",
      "metadata": {
        "id": "a9feb7b4"
      },
      "source": [
        "## Performance Comparison\n",
        "\n",
        "Measure the time it takes to do inference on twenty images. This gives an indication of performance. For more accurate benchmarking, use the [OpenVINO Benchmark Tool](https://docs.openvino.ai/latest/openvino_inference_engine_tools_benchmark_tool_README.html). Note that many optimizations are possible to improve the performance. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "835b75e8",
      "metadata": {
        "id": "835b75e8"
      },
      "outputs": [],
      "source": [
        "num_images = 20\n",
        "\n",
        "start = time.perf_counter()\n",
        "for _ in range(num_images):\n",
        "    compiled_model_onnx([normalized_input_image])\n",
        "end = time.perf_counter()\n",
        "time_onnx = end - start\n",
        "print(\n",
        "    f\"ONNX model in Inference Engine/CPU: {time_onnx/num_images:.3f} \"\n",
        "    f\"seconds per image, FPS: {num_images/time_onnx:.2f}\"\n",
        ")\n",
        "\n",
        "start = time.perf_counter()\n",
        "for _ in range(num_images):\n",
        "    compiled_model_ir([input_image])\n",
        "end = time.perf_counter()\n",
        "time_ir = end - start\n",
        "print(\n",
        "    f\"IR model in Inference Engine/CPU: {time_ir/num_images:.3f} \"\n",
        "    f\"seconds per image, FPS: {num_images/time_ir:.2f}\"\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    start = time.perf_counter()\n",
        "    for _ in range(num_images):\n",
        "        model(torch.as_tensor(input_image).float())\n",
        "    end = time.perf_counter()\n",
        "    time_torch = end - start\n",
        "print(\n",
        "    f\"PyTorch model on CPU: {time_torch/num_images:.3f} seconds per image, \"\n",
        "    f\"FPS: {num_images/time_torch:.2f}\"\n",
        ")\n",
        "\n",
        "if \"GPU\" in ie.available_devices:\n",
        "    compiled_model_onnx_gpu = ie.compile_model(model=model_onnx, device_name=\"GPU\")\n",
        "    start = time.perf_counter()\n",
        "    for _ in range(num_images):\n",
        "        compiled_model_onnx_gpu([input_image])\n",
        "    end = time.perf_counter()\n",
        "    time_onnx_gpu = end - start\n",
        "    print(\n",
        "        f\"ONNX model in Inference Engine/GPU: {time_onnx_gpu/num_images:.3f} \"\n",
        "        f\"seconds per image, FPS: {num_images/time_onnx_gpu:.2f}\"\n",
        "    )\n",
        "\n",
        "    compiled_model_ir_gpu = ie.compile_model(model=model_ir, device_name=\"GPU\")\n",
        "    start = time.perf_counter()\n",
        "    for _ in range(num_images):\n",
        "        compiled_model_ir_gpu([input_image])\n",
        "    end = time.perf_counter()\n",
        "    time_ir_gpu = end - start\n",
        "    print(\n",
        "        f\"IR model in Inference Engine/GPU: {time_ir_gpu/num_images:.3f} \"\n",
        "        f\"seconds per image, FPS: {num_images/time_ir_gpu:.2f}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6ca50ca",
      "metadata": {
        "id": "c6ca50ca"
      },
      "source": [
        "**Show Device Information**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab884bb4",
      "metadata": {
        "id": "ab884bb4"
      },
      "outputs": [],
      "source": [
        "devices = ie.available_devices\n",
        "for device in devices:\n",
        "    device_name = ie.get_property(device_name=device, name=\"FULL_DEVICE_NAME\")\n",
        "    print(f\"{device}: {device_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ä¸‹è¼‰æ¨¡åž‹æª”æ¡ˆ\n"
      ],
      "metadata": {
        "id": "Nq9zg-fOtd4e"
      },
      "id": "Nq9zg-fOtd4e"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"model/fastseg1024.onnx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "t7h3JeJWthBp",
        "outputId": "2f18cd2e-41c0-49d6-a96a-0a63629a422d"
      },
      "id": "t7h3JeJWthBp",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8139008e-6eca-4948-bc32-6ab8cd4dea9a\", \"fastseg1024.onnx\", 13224524)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ä½¿ç”¨Netronè§€çœ‹æ¨¡åž‹\n"
      ],
      "metadata": {
        "id": "fEmBx3l8tMrc"
      },
      "id": "fEmBx3l8tMrc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. [https://github.com/lutzroeder/netron](https://github.com/lutzroeder/netron)\n",
        "2. [https://netron.app/](https://netron.app/)"
      ],
      "metadata": {
        "id": "N-fHj5c9tRh-"
      },
      "id": "N-fHj5c9tRh-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Camera Capture\n",
        "Using a webcam to capture images for processing on the runtime."
      ],
      "metadata": {
        "id": "lneZndT3uWWC"
      },
      "id": "lneZndT3uWWC"
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import display\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = display.Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display.display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "metadata": {
        "id": "A7w8lr8FuQ-S"
      },
      "id": "A7w8lr8FuQ-S",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  filename = take_photo()\n",
        "  print('Saved to {}'.format(filename))\n",
        "  \n",
        "  # Show the image which was just taken.\n",
        "  display.display(display.Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ],
      "metadata": {
        "id": "B-2txhlKubs7"
      },
      "id": "B-2txhlKubs7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "tags": [],
        "id": "py0902YnuRGo"
      },
      "outputs": [],
      "source": [
        "image_filename = \"./photo.jpg\"\n",
        "# image_filename = \"/content/sample-images/people.jpg\"\n",
        "# image_filename = \"/content/sample-images/sitting-on-car.jpg\"\n",
        "image = cv2.cvtColor(cv2.imread(image_filename), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "resized_image = cv2.resize(image, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
        "normalized_image = normalize(resized_image)\n",
        "\n",
        "# Convert the resized images to network input shape\n",
        "input_image = np.expand_dims(np.transpose(resized_image, (2, 0, 1)), 0)\n",
        "normalized_input_image = np.expand_dims(np.transpose(normalized_image, (2, 0, 1)), 0)"
      ],
      "id": "py0902YnuRGo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWx6qRO3uRGp"
      },
      "source": [
        "### Load the OpenVINO IR Network and Run Inference on the ONNX model\n",
        "\n",
        "Inference Engine can load ONNX models directly. We first load the ONNX model, do inference and show the results. After that we load the model that was converted to Intermediate Representation (IR) with Model Optimizer and do inference on that model and show the results on an image from [Mapillary Vistas](https://www.mapillary.com/dataset/vistas).\n",
        "\n",
        "#### 1. ONNX Model in Inference Engine"
      ],
      "id": "sWx6qRO3uRGp"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "tags": [],
        "id": "m6SumeskuRGp"
      },
      "outputs": [],
      "source": [
        "# Load network to Inference Engine\n",
        "ie = Core()\n",
        "model_onnx = ie.read_model(model=onnx_path)\n",
        "compiled_model_onnx = ie.compile_model(model=model_onnx, device_name=\"CPU\")\n",
        "\n",
        "output_layer_onnx = compiled_model_onnx.output(0)\n",
        "\n",
        "# Run inference on the input image\n",
        "res_onnx = compiled_model_onnx([normalized_input_image])[output_layer_onnx]"
      ],
      "id": "m6SumeskuRGp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ZsQG4BhXuRGp"
      },
      "outputs": [],
      "source": [
        "# Convert network result to segmentation map and display the result\n",
        "result_mask_onnx = np.squeeze(np.argmax(res_onnx, axis=1)).astype(np.uint8)\n",
        "viz_result_image(\n",
        "    image,\n",
        "    segmentation_map_to_image(result_mask_onnx, CityScapesSegmentation.get_colormap()),\n",
        "    resize=True,\n",
        ")"
      ],
      "id": "ZsQG4BhXuRGp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiHpA6szuRGp"
      },
      "source": [
        "#### 2. IR Model in Inference Engine"
      ],
      "id": "DiHpA6szuRGp"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "tags": [],
        "id": "RW3jDAjquRGq"
      },
      "outputs": [],
      "source": [
        "# Load the network in Inference Engine\n",
        "ie = Core()\n",
        "model_ir = ie.read_model(model=ir_path)\n",
        "compiled_model_ir = ie.compile_model(model=model_ir, device_name=\"CPU\")\n",
        "\n",
        "# Get input and output layers\n",
        "output_layer_ir = compiled_model_ir.output(0)\n",
        "\n",
        "# Run inference on the input image\n",
        "res_ir = compiled_model_ir([input_image])[output_layer_ir]"
      ],
      "id": "RW3jDAjquRGq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "lusxmfXhuRGq"
      },
      "outputs": [],
      "source": [
        "result_mask_ir = np.squeeze(np.argmax(res_ir, axis=1)).astype(np.uint8)\n",
        "viz_result_image(\n",
        "    image,\n",
        "    segmentation_map_to_image(result=result_mask_ir, colormap=CityScapesSegmentation.get_colormap()),\n",
        "    resize=True,\n",
        ")"
      ],
      "id": "lusxmfXhuRGq"
    },
    {
      "cell_type": "markdown",
      "id": "d8ed2610",
      "metadata": {
        "id": "d8ed2610"
      },
      "source": [
        "## References\n",
        "\n",
        "* [Fastseg](https://github.com/ekzhang/fastseg)\n",
        "* [PIP install openvino-dev](https://github.com/openvinotoolkit/openvino/blob/releases/2021/3/docs/install_guides/pypi-openvino-dev.md)\n",
        "* [OpenVINO ONNX support](https://docs.openvino.ai/2021.4/openvino_docs_IE_DG_ONNX_Support.html)\n",
        "* [Model Optimizer Documentation](https://docs.openvino.ai/latest/openvino_docs_MO_DG_prepare_model_convert_model_Converting_Model_General.html)\n"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "ae617ccb002f72b3ab6d0069d721eac67ac2a969e83c083c4321cfcab0437cd1"
    },
    "kernelspec": {
      "display_name": "openvino_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "colab": {
      "name": "102-pytorch-onnx-to-openvino.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}